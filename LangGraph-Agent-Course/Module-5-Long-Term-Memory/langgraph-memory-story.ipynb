{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db0362c0",
   "metadata": {},
   "source": [
    "# ***Basics Implementation of the Long-Term-Memory in ChatBot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6619ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed845d7f",
   "metadata": {},
   "source": [
    "## Define the in memory store using build in class `InMemoryStore`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945684e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a34d30",
   "metadata": {},
   "source": [
    "When storing objects (e.g., memories) in the [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore), we provide:\n",
    "\n",
    "- The `namespace` for the object, a tuple (similar to directories)\n",
    "- the object `key` (similar to filenames)\n",
    "- the object `value` (similar to file contents)\n",
    "\n",
    "We use the [put](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.put) method to save an object to the store by `namespace` and `key`.\n",
    "\n",
    "![langgraph_store.png](inmemory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e6bf9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Namespace for the memory\n",
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memory\")\n",
    "\n",
    "# save a memory to namespace as key and values\n",
    "key = str(uuid.uuid4())\n",
    "\n",
    "# this is the memory context like value of the memory\n",
    "value = {\n",
    "    'food_preference': \"I like beef\"\n",
    "}\n",
    "\n",
    "## now save this info into the memory\n",
    "in_memory_store.put(namespace=namespace_for_memory, key=key, value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897b136",
   "metadata": {},
   "source": [
    "#### we use `search` to retrieve the object from the store by namespace\n",
    "- This is return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "476402af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search\n",
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "type(memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d34713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['1', 'memory'],\n",
       " 'key': '132405f5-c8a3-47b5-a913-cb022f8f0e36',\n",
       " 'value': {'food_preference': 'I like beef'},\n",
       " 'created_at': '2025-05-12T06:38:58.352749+00:00',\n",
       " 'updated_at': '2025-05-12T06:38:58.352749+00:00',\n",
       " 'score': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Metadata\n",
    "memories[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8d861fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132405f5-c8a3-47b5-a913-cb022f8f0e36 {'food_preference': 'I like beef'}\n"
     ]
    }
   ],
   "source": [
    "print(memories[0].key, memories[0].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68349af",
   "metadata": {},
   "source": [
    "We can also use [get](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore.get) to retrieve an object by `namespace` and `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02db6ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['1', 'memory'],\n",
       " 'key': '132405f5-c8a3-47b5-a913-cb022f8f0e36',\n",
       " 'value': {'food_preference': 'I like beef'},\n",
       " 'created_at': '2025-05-12T06:38:58.352749+00:00',\n",
       " 'updated_at': '2025-05-12T06:38:58.352749+00:00'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the memory by namespace and key\n",
    "memory = in_memory_store.get(namespace_for_memory, key)\n",
    "memory.dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff372b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like beef'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_content = memory.value.get(\"food_preference\")\n",
    "memory_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc139d4e",
   "metadata": {},
   "source": [
    "# ***Time to Build Chatbot with long-term-memory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aae6224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\", groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734b6ef",
   "metadata": {},
   "source": [
    "## Necessary Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa8f604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chatbot instruction\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful assistant with memory that provides information about the user. \n",
    "If you have memory for this user, use it to personalize your responses.\n",
    "Here is the memory (it may be empty): {memory}\"\"\"\n",
    "\n",
    "# Create new memory from the chat history and any existing memory\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ed3cd",
   "metadata": {},
   "source": [
    "## **Call model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1afae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Load memory from the store and use it to personalize the chatbot's response\"\"\"\n",
    "    \n",
    "    ## get the user id\n",
    "    user_id = config['configurable']['user_id']\n",
    "    \n",
    "    ## retriever memory form the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    key = \"user_memory\"\n",
    "    existing_memory = store.get(namespace=namespace, key=key)\n",
    "    \n",
    "    ## extract the existing memory and add as the prefix\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get(\"memory\")\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "        \n",
    "    ## format the memory to the system prompt\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(memory=existing_memory_content)\n",
    "    \n",
    "    ## respond using memory as well as the chat history\n",
    "    response = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "    \n",
    "    return {\n",
    "        'messages': response\n",
    "    }\n",
    "    \n",
    "    \n",
    "## Write the memory\n",
    "    \n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    \"\"\"Reflect on the chat history and save a memory to the store\"\"\"\n",
    "    \n",
    "    # Get the user ID from the config\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    # Retrieve existing memory from the store\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memory = store.get(namespace, \"user_memory\")\n",
    "        \n",
    "    # Extract the memory\n",
    "    if existing_memory:\n",
    "        existing_memory_content = existing_memory.value.get('memory')\n",
    "    else:\n",
    "        existing_memory_content = \"No existing memory found.\"\n",
    "    \n",
    "    # format the memory in the system prompt\n",
    "    system_msg = CREATE_MEMORY_INSTRUCTION.format(memory = existing_memory_content)\n",
    "    new_memory = model.invoke([SystemMessage(content=system_msg)]+state['messages'])\n",
    "    \n",
    "    ## overwrite the existing memory in the store\n",
    "    key = \"user_memory\"\n",
    "    \n",
    "    ## write the value as a dictionary with a memory key\n",
    "    store.put(namespace, key, {\"memory\": new_memory.content})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949c9787",
   "metadata": {},
   "source": [
    "## Define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1adde438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "\n",
    "builder.add_edge(START, 'call_model')\n",
    "builder.add_edge('call_model', 'write_memory')\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "## store for long-term memory(across thread) memory\n",
    "across_thread_memory = InMemoryStore()\n",
    "\n",
    "## checkpointer for short-term (within thread) memory\n",
    "within_thread_memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile(checkpointer=within_thread_memory, store=across_thread_memory)\n",
    "\n",
    "## display the graph\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbedc14",
   "metadata": {},
   "source": [
    "When we interact with the chatbot, we supply two things:\n",
    "\n",
    "1. `Short-term (within-thread) memory`: A `thread ID` for persisting the chat history.\n",
    "2. `Long-term (cross-thread) memory`: A `user ID` to namespace long-term memories to the user.\n",
    "\n",
    "Let's see how these work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a7a99fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Assalamualikum, my name is AlAmin\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Wa alaikumussalam, AlAmin! It's nice to meet you. How are you today?\n"
     ]
    }
   ],
   "source": [
    "# We supply a thread ID for short-term (within-thread) memory\n",
    "# We supply a user ID for long-term (across-thread) memory \n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Assalamualikum, my name is AlAmin\")]\n",
    "\n",
    "## run the graph with streaming mode\n",
    "for chunk in graph.stream(\n",
    "    {\n",
    "        \"messages\": input_messages\n",
    "    },\n",
    "    config=config,\n",
    "    stream_mode='values'\n",
    "):\n",
    "    chunk['messages'][-1].pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85ccb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Allhamdulliha, I like to bike around Bashundhra-r/a, Dhaka\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Alhamdulillah! Bashundhara Residential Area in Dhaka is a lovely place to bike around. The roads are relatively smooth, and it's a great way to stay active and enjoy the outdoors. Do you have a favorite route or spot in Bashundhara that you like to visit while biking?\n"
     ]
    }
   ],
   "source": [
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Allhamdulliha, I like to bike around Bashundhra-r/a, Dhaka\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfc3a0c",
   "metadata": {},
   "source": [
    "We're using the `MemorySaver` checkpointer for within-thread memory.\n",
    "\n",
    "This saves the chat history to the thread.\n",
    "\n",
    "We can look at the chat history saved to the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "885d7592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Assalamualikum, my name is AlAmin\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Wa alaikumussalam, AlAmin! It's nice to meet you. How are you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Allhamdulliha, I like to bike around Bashundhra-r/a, Dhaka\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Alhamdulillah! Bashundhara Residential Area in Dhaka is a lovely place to bike around. The roads are relatively smooth, and it's a great way to stay active and enjoy the outdoors. Do you have a favorite route or spot in Bashundhara that you like to visit while biking?\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state = graph.get_state(thread).values\n",
    "for m in state[\"messages\"]: \n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c8a01",
   "metadata": {},
   "source": [
    "Recall that we compiled the graph with our the store: \n",
    "\n",
    "```python\n",
    "across_thread_memory = InMemoryStore()\n",
    "```\n",
    "\n",
    "And, we added a node to the graph (`write_memory`) that reflects on the chat history and saves a memory to the store.\n",
    "\n",
    "We can to see if the memory was saved to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afd203ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'namespace': ['memory', '1'],\n",
       " 'key': 'user_memory',\n",
       " 'value': {'memory': ' \\n\\n**Current User Information:**\\n* Name: AlAmin\\n* Location: Bashundhara Residential Area, Dhaka\\n* Interests/Hobbies: Biking'},\n",
       " 'created_at': '2025-05-12T06:39:02.331949+00:00',\n",
       " 'updated_at': '2025-05-12T06:39:02.331949+00:00'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Namespace for the memory to save\n",
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id)\n",
    "existing_memory = across_thread_memory.get(namespace, \"user_memory\")\n",
    "existing_memory.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7ed83",
   "metadata": {},
   "source": [
    "Now, let's kick off a *new thread* with the *same user ID*.\n",
    "\n",
    "We should see that the chatbot remembered the user's profile and used it to personalize the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a00e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Assalamualikum! Where would you recommend that I go biking?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Wa alaikumussalam! Ah, biking is a great hobby, AlAmin! I'm glad you asked.\n",
      "\n",
      "Considering you're from Bashundhara Residential Area, Dhaka, I'd recommend the following biking spots:\n",
      "\n",
      "1. **Hatirjheel Lake**: A scenic and relatively flat route with a dedicated bike lane. You can enjoy the lake's views and the city's vibrant atmosphere.\n",
      "2. **Banani Lake**: Another beautiful spot with a walking and biking path. The lake's surroundings are perfect for a leisurely bike ride.\n",
      "3. **Gulshan Lake Park**: A peaceful spot with a designated bike path. You can enjoy the serene environment and some exercise.\n",
      "4. **The Liberation War Museum to Suhrawardy Uddan route**: A relatively flat and scenic route that takes you through some of Dhaka's historic areas.\n",
      "\n",
      "If you're looking for a more challenging ride, you could consider:\n",
      "\n",
      "1. **The hills of Mirpur**: Offers some uphill climbs and scenic views of the city.\n",
      "2. **The bike trails near the Turag River**: A bit farther from Bashundhara, but worth the ride for the scenic views and varied terrain.\n",
      "\n",
      "Remember to always wear safety gear, follow traffic rules, and be mindful of road conditions.\n",
      "\n",
      "Which one of these options sounds appealing to you, AlAmin?\n"
     ]
    }
   ],
   "source": [
    "# We supply a user ID for across-thread memory as well as a new thread ID\n",
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n",
    "\n",
    "# User input \n",
    "input_messages = [HumanMessage(content=\"Assalamualikum! Where would you recommend that I go biking?\")]\n",
    "\n",
    "# Run the graph\n",
    "for chunk in graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af832d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_aca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
